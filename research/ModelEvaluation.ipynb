{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\QA-BOT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\END to END Proj\\QA-BOT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
    "import torch\n",
    "from src.QABOT import logger\n",
    "from src.QABOT.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    tfidf_model_dir: Path\n",
    "    distilbert_model_dir: Path\n",
    "    metrics_file: Path\n",
    "    test_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.QABOT.constant import *\n",
    "from src.QABOT.utils.common import read_yaml,create_directories \n",
    "import json\n",
    "from src.QABOT import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "     ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tfidf_model_dir=Path(config.tfidf_model_dir),\n",
    "            distilbert_model_dir=Path(config.distilbert_model_dir),\n",
    "            metrics_file=Path(config.metrics_file),\n",
    "            test_data_path=Path(config.test_data_path)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define any custom preprocessing functions needed for TF-IDF\n",
    "def preprocess(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        create_directories([self.config.root_dir])\n",
    "        self._should_evaluate = not os.path.exists(self.config.root_dir)  # Only evaluate if folder doesn't exist\n",
    "\n",
    "    def _metrics_exist(self) -> bool:\n",
    "        \"\"\"Check if metrics file exists and is valid\"\"\"\n",
    "        if not os.path.exists(self.config.metrics_file):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(self.config.metrics_file, \"r\") as f:\n",
    "                metrics = json.load(f)\n",
    "                required_keys = {\"tfidf\", \"distilbert\", \"test_samples\"}\n",
    "                return all(key in metrics for key in required_keys)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _load_tfidf_model(self):\n",
    "        \"\"\"Load TF-IDF vectorizer and sentences\"\"\"\n",
    "        vectorizer = joblib.load(self.config.tfidf_model_dir / \"tfidf_vectorizer.pkl\")\n",
    "        with open(self.config.tfidf_model_dir / \"tfidf_sentences.json\", \"r\") as f:\n",
    "            sentences = json.load(f)\n",
    "        return vectorizer, sentences\n",
    "\n",
    "    def _load_distilbert_model(self):\n",
    "        \"\"\"Load DistilBERT model and tokenizer\"\"\"\n",
    "        model = DistilBertForQuestionAnswering.from_pretrained(\n",
    "            self.config.distilbert_model_dir\n",
    "        ).to(self.device)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "            self.config.distilbert_model_dir\n",
    "        )\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _load_test_data(self):\n",
    "        \"\"\"Load test data in SQuAD format\"\"\"\n",
    "        with open(self.config.test_data_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _flatten_squad(self, squad_data):\n",
    "        \"\"\"Convert SQuAD format to list of QA pairs\"\"\"\n",
    "        rows = []\n",
    "        for article in squad_data[\"data\"]:\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"]\n",
    "                for qa in para[\"qas\"]:\n",
    "                    if \"answers\" not in qa or len(qa[\"answers\"]) == 0:\n",
    "                        continue\n",
    "                    ans = qa[\"answers\"][0]\n",
    "                    rows.append({\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"context\": context,\n",
    "                        \"answer_text\": ans[\"text\"],\n",
    "                        \"answer_start\": ans[\"answer_start\"]\n",
    "                    })\n",
    "        return rows\n",
    "\n",
    "    def _exact_match(self, pred: str, truth: str) -> int:\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "        pred = pred.strip().lower()\n",
    "        truth = truth.strip().lower()\n",
    "        return int(pred == truth)\n",
    "\n",
    "    def evaluate_tfidf(self, test_data):\n",
    "        \"\"\"Evaluate TF-IDF model using exact match\"\"\"\n",
    "        vectorizer, sentences = self._load_tfidf_model()\n",
    "        em_scores = []\n",
    "        \n",
    "        for item in test_data:\n",
    "            question = item[\"question\"]\n",
    "            gold_answer = item[\"answer_text\"]\n",
    "            \n",
    "            # Vectorize question and find most similar sentence\n",
    "            q_vec = vectorizer.transform([question])\n",
    "            sims = cosine_similarity(q_vec, vectorizer.transform(sentences))[0]\n",
    "            pred_answer = sentences[np.argmax(sims)].strip()\n",
    "            \n",
    "            em_scores.append(self._exact_match(pred_answer, gold_answer))\n",
    "        \n",
    "        return {\"exact_match\": float(np.mean(em_scores))}\n",
    "\n",
    "    def evaluate_distilbert(self, test_data):\n",
    "        \"\"\"Evaluate DistilBERT model using exact match\"\"\"\n",
    "        model, tokenizer = self._load_distilbert_model()\n",
    "        em_scores = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for item in test_data:\n",
    "                question = item[\"question\"]\n",
    "                context = item[\"context\"]\n",
    "                gold_answer = item[\"answer_text\"]\n",
    "                \n",
    "                # Get model prediction\n",
    "                inputs = tokenizer(question, context, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                # Convert to answer text\n",
    "                start = torch.argmax(outputs.start_logits)\n",
    "                end = torch.argmax(outputs.end_logits) + 1\n",
    "                pred_answer = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start:end]))\n",
    "                \n",
    "                em_scores.append(self._exact_match(pred_answer, gold_answer))\n",
    "        \n",
    "        return {\"exact_match\": float(np.mean(em_scores))}\n",
    "\n",
    "    def evaluate_and_save(self):\n",
    "        \"\"\"Run evaluation only if model_evaluation folder doesn't exist\"\"\"\n",
    "        try:\n",
    "            # Skip if evaluation folder exists\n",
    "            if os.path.exists(self.config.root_dir):\n",
    "                logger.info(f\"Evaluation folder {self.config.root_dir} exists. Skipping evaluation.\")\n",
    "                if self._metrics_exist():\n",
    "                    with open(self.config.metrics_file, \"r\") as f:\n",
    "                        return json.load(f)\n",
    "                return {\"status\": \"evaluation_skipped\", \"reason\": \"folder_exists\"}\n",
    "\n",
    "            # Proceed with evaluation\n",
    "            logger.info(\" Starting new evaluation...\")\n",
    "            test_data = self._flatten_squad(self._load_test_data())\n",
    "            \n",
    "            # Evaluate both models\n",
    "            tfidf_metrics = self.evaluate_tfidf(test_data)\n",
    "            distilbert_metrics = self.evaluate_distilbert(test_data)\n",
    "            \n",
    "            # Combine and save metrics\n",
    "            metrics = {\n",
    "                \"tfidf\": tfidf_metrics,\n",
    "                \"distilbert\": distilbert_metrics,\n",
    "                \"test_samples\": len(test_data)\n",
    "            }\n",
    "            \n",
    "            with open(self.config.metrics_file, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "            \n",
    "            logger.info(f\" Evaluation complete. Metrics saved to {self.config.metrics_file}\")\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" Evaluation failed: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-13 18:10:22,879: INFO: common: created directory at: artifacts\\model_evaluation]\n",
      "[2025-08-13 18:10:22,881: INFO: 2599859575: Evaluation folder artifacts\\model_evaluation exists. Skipping evaluation.]\n",
      "{'status': 'evaluation_skipped', 'reason': 'folder_exists'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    config = ModelEvaluationConfig(\n",
    "        root_dir=Path(\"artifacts/model_evaluation\"),\n",
    "        tfidf_model_dir=Path(\"artifacts/model_trainer/tfidf\"),\n",
    "        distilbert_model_dir=Path(\"artifacts/model_trainer/distilbert\"),\n",
    "        metrics_file=Path(\"artifacts/model_evaluation/metrics.json\"),\n",
    "        test_data_path=Path(\"artifacts/data_ingestion/dev-v1.1.json\")\n",
    "    )\n",
    "    \n",
    "    evaluator = ModelEvaluator(config)\n",
    "    metrics = evaluator.evaluate_and_save()\n",
    "    print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
