{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\QA-BOT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\END to END Proj\\QA-BOT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import os\n",
    "from src.QABOT import logger\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    TRAIN_FILE: Path\n",
    "    DEV_FILE: Path\n",
    "    MAX_LEN: int\n",
    "    DOC_STRIDE: int\n",
    "    PARA_VECTORIZER_FILE: Path\n",
    "    SENT_VECTORIZER_FILE: Path\n",
    "    PARAGRAPHS_FILE: Path\n",
    "    SENTENCES_FILE: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.QABOT.constant import *\n",
    "from src.QABOT.utils.common import read_yaml,create_directories \n",
    "import json\n",
    "from src.QABOT import logger\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "     ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            TRAIN_FILE=Path(config.TRAIN_FILE),\n",
    "            DEV_FILE=Path(config.DEV_FILE),\n",
    "            MAX_LEN=self.params.Transformation.MAX_LEN,\n",
    "            DOC_STRIDE=self.params.Transformation.DOC_STRIDE,\n",
    "            PARA_VECTORIZER_FILE=Path(config.PARA_VECTORIZER_FILE),\n",
    "            SENT_VECTORIZER_FILE=Path(config.SENT_VECTORIZER_FILE),\n",
    "            PARAGRAPHS_FILE=Path(config.PARAGRAPHS_FILE),\n",
    "            SENTENCES_FILE=Path(config.SENTENCES_FILE)\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('tokenizers/punkt_tab')  # Specifically check for punkt_tab\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('punkt_tab', quiet=True)  # Download punkt_tab specifically\n",
    "          \n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "            \"distilbert-base-uncased\",\n",
    "            use_auth_token=\"\"  # Replace with your actual token\n",
    "        )\n",
    "\n",
    "    def flatten_squad(self, squad_js: Dict) -> List[Dict]:\n",
    "        \"\"\"Convert SQuAD format to flattened QA pairs\"\"\"\n",
    "        rows = []\n",
    "        for article in squad_js[\"data\"]:\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"]\n",
    "                for qa in para[\"qas\"]:\n",
    "                    if \"answers\" not in qa or len(qa[\"answers\"]) == 0:\n",
    "                        continue\n",
    "                    ans = qa[\"answers\"][0]\n",
    "                    rows.append({\n",
    "                        \"id\": qa[\"id\"],\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"context\": context,\n",
    "                        \"answer_text\": ans[\"text\"],\n",
    "                        \"answer_start\": ans[\"answer_start\"],\n",
    "                    })\n",
    "        return rows\n",
    "\n",
    "    def collect_paragraphs(self, squad_list: List[Dict]) -> List[str]:\n",
    "        \"\"\"Extract all unique paragraphs from SQuAD data\"\"\"\n",
    "        paras = []\n",
    "        for article in squad_list:\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                paras.append(para[\"context\"])\n",
    "        return list(dict.fromkeys(paras))  # Deduplicate\n",
    "\n",
    "    def build_tfidf_retriever(self, paragraphs: List[str]) -> Tuple:\n",
    "        \"\"\"Create TF-IDF vectorizer and vectorize paragraphs\"\"\"\n",
    "        vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "        tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "        return vectorizer, tfidf_matrix\n",
    "\n",
    "    def build_sentence_bank(self, paragraphs: List[str]) -> Tuple:\n",
    "        \"\"\"Create sentence-level TF-IDF index\"\"\"\n",
    "        all_sentences = []\n",
    "        sent2para_idx = []\n",
    "        for p_i, p in enumerate(paragraphs):\n",
    "            sents = sent_tokenize(p)\n",
    "            all_sentences.extend(sents)\n",
    "            sent2para_idx.extend([p_i]*len(sents))\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "        return vectorizer, tfidf_matrix, all_sentences\n",
    "\n",
    "    def prepare_features(self, rows: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Convert QA pairs to model input features with overflow handling\"\"\"\n",
    "        features = []\n",
    "        for r in tqdm(rows, desc=\"Tokenizing examples\"):\n",
    "            question = r[\"question\"]\n",
    "            context = r[\"context\"]\n",
    "            answer_text = r[\"answer_text\"]\n",
    "            answer_start = r[\"answer_start\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            tokenized = self.tokenizer(\n",
    "                question,\n",
    "                context,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.config.MAX_LEN,\n",
    "                stride=self.config.DOC_STRIDE,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            for i in range(len(tokenized[\"input_ids\"])):\n",
    "                offsets = tokenized[\"offset_mapping\"][i]\n",
    "                input_ids = tokenized[\"input_ids\"][i]\n",
    "                attention_mask = tokenized[\"attention_mask\"][i]\n",
    "                seq_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "                # Find context span\n",
    "                context_start = next((idx for idx, s in enumerate(seq_ids) if s == 1), None)\n",
    "                context_end = next((idx for idx in reversed(range(len(seq_ids))) if seq_ids[idx] == 1), None)\n",
    "\n",
    "                if context_start is None or context_end is None:\n",
    "                    continue\n",
    "\n",
    "                # Locate answer span\n",
    "                cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
    "                start_token = end_token = cls_index\n",
    "\n",
    "                for idx in range(context_start, context_end + 1):\n",
    "                    start_char, end_char = offsets[idx]\n",
    "                    if start_char <= answer_start and end_char >= answer_start:\n",
    "                        start_token = idx\n",
    "                    if start_char <= answer_end-1 and end_char >= answer_end-1:\n",
    "                        end_token = idx\n",
    "\n",
    "                if start_token != cls_index or end_token != cls_index:\n",
    "                    features.append({\n",
    "                        \"input_ids\": torch.tensor(input_ids),\n",
    "                        \"attention_mask\": torch.tensor(attention_mask),\n",
    "                        \"start_positions\": torch.tensor(start_token),\n",
    "                        \"end_positions\": torch.tensor(end_token),\n",
    "                    })\n",
    "        return features\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"Execute full transformation pipeline\"\"\"\n",
    "        try:\n",
    "            # Load raw data\n",
    "            with open(self.config.TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                train_squad = json.load(f)\n",
    "            with open(self.config.DEV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                dev_squad = json.load(f)\n",
    "\n",
    "            # Flatten QA pairs\n",
    "            train_rows = self.flatten_squad(train_squad)\n",
    "            dev_rows = self.flatten_squad(dev_squad)\n",
    "            logger.info(f\"Flattened {len(train_rows)} train and {len(dev_rows)} dev examples\")\n",
    "\n",
    "            # Build retrieval corpus\n",
    "            all_paragraphs = self.collect_paragraphs(train_squad[\"data\"]) + self.collect_paragraphs(dev_squad[\"data\"])\n",
    "            para_vectorizer, para_tfidf = self.build_tfidf_retriever(all_paragraphs)\n",
    "            sent_vectorizer, sent_tfidf, all_sentences = self.build_sentence_bank(all_paragraphs)\n",
    "            logger.info(f\"Built TF-IDF retriever with {len(all_paragraphs)} paragraphs and {len(all_sentences)} sentences\")\n",
    "\n",
    "            # Prepare model features\n",
    "            train_features = self.prepare_features(train_rows)\n",
    "            dev_features = self.prepare_features(dev_rows)\n",
    "            logger.info(f\"Prepared {len(train_features)} train and {len(dev_features)} dev features\")\n",
    "\n",
    "            # Save artifacts\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "            joblib.dump(para_vectorizer, self.config.PARA_VECTORIZER_FILE)\n",
    "            joblib.dump(sent_vectorizer, self.config.SENT_VECTORIZER_FILE)\n",
    "            with open(self.config.PARAGRAPHS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_paragraphs, f, ensure_ascii=False)\n",
    "            with open(self.config.SENTENCES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_sentences, f, ensure_ascii=False)\n",
    "            \n",
    "            return QADataset(train_features), QADataset(dev_features)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for QA features\"\"\"\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-13 16:18:41,593: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-13 16:18:41,593: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-13 16:18:41,593: INFO: common: created directory at: artifacts]\n",
      "[2025-08-13 16:18:41,593: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\END to END Proj\\QA-BOT\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1908: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-13 16:18:43,391: INFO: 232401020: Flattened 87599 train and 10570 dev examples]\n",
      "[2025-08-13 16:18:51,273: INFO: 232401020: Built TF-IDF retriever with 20958 paragraphs and 104034 sentences]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing examples: 100%|██████████| 87599/87599 [02:03<00:00, 710.62it/s] \n",
      "Tokenizing examples: 100%|██████████| 10570/10570 [00:15<00:00, 664.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-13 16:21:10,443: INFO: 232401020: Prepared 87790 train and 10616 dev features]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-13 16:21:12,825: INFO: 4003408384:  Data transformation completed successfully]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config_manager = ConfigurationManager()\n",
    "        transformation_config = config_manager.get_data_transformation_config()\n",
    "        transformer = DataTransformation(config=transformation_config)\n",
    "        train_dataset, dev_dataset = transformer.transform()\n",
    "        logger.info(\" Data transformation completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data transformation failed: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
